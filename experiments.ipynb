{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e18a0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8daec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched logs from API\n",
      "Step 1 complete: 100 logs fetched\n"
     ]
    }
   ],
   "source": [
    "# STEP 1\n",
    "# Fetch logs from Apache API with retry logic\n",
    "api_url = \"https://apache-api.onrender.com/logs\"\n",
    "logs = []\n",
    "\n",
    "for attempt in range(3):\n",
    "    response = requests.get(api_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Successfully fetched logs from API\")\n",
    "        json_data = json.loads(response.text)\n",
    "        logs = json_data[\"raw_logs\"]\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Failed to fetch logs from API. Status code: {response.status_code}\")\n",
    "        if attempt < 2:\n",
    "            print(\"Retrying...\")\n",
    "        else:\n",
    "            print(\"All attempts to fetch logs from API failed\")\n",
    "            \n",
    "print(f\"Step 1 complete: {len(logs)} logs fetched\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a4184c",
   "metadata": {},
   "source": [
    "##### Transformation\n",
    "- Parse fields using a regex-based parser (parser.py).\n",
    "- Deduplicate records based on a hash of IP + timestamp + request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "638177a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 100 logs\n",
      "Duplicates removed: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "# Parse Apache log lines into structured data(converts raw text logs into organized data that is easier to work with)\n",
    "log_pattern = re.compile(\n",
    "    r'(?P<ip>\\S+) - - \\[(?P<timestamp>[^\\]]+)\\] \"(?P<method>\\S+) (?P<url>\\S+) (?P<protocol>[^\"]+)\" (?P<status>\\d{3}) (?P<size>\\d+|-)'\n",
    ")\n",
    "parsed_logs = []\n",
    "seen_hashes = set()  \n",
    "duplicates_count = 0\n",
    "\n",
    "for line in logs:\n",
    "    match = log_pattern.match(line)\n",
    "    if match:\n",
    "        log_entry = match.groupdict()\n",
    "\n",
    "       # Checks for size field in '-', before converting to integer\n",
    "        if log_entry['size'] == '-':\n",
    "            log_entry['size'] = 0\n",
    "        else:\n",
    "            log_entry['size'] = int(log_entry['size']) \n",
    "            \n",
    "        # Convert status to integers as regex captures everything as strings\n",
    "        log_entry['status'] = int(log_entry['status']) \n",
    "\n",
    "        # Deduplication: Create hash based on IP + timestamp + request\n",
    "        hash_input = f\"{log_entry['ip']}{log_entry['timestamp']}{log_entry['method']}{log_entry['url']}\"\n",
    "        log_hash = hashlib.md5(hash_input.encode()).hexdigest()\n",
    "            \n",
    "        # Check for duplicates\n",
    "        if log_hash in seen_hashes:\n",
    "            duplicates_count += 1\n",
    "            continue  \n",
    "\n",
    "        # Add hash to seen set and store in log entry\n",
    "        seen_hashes.add(log_hash)\n",
    "        log_entry['log_hash'] = log_hash\n",
    "        \n",
    "        parsed_logs.append(log_entry)\n",
    "\n",
    "print(f\"Parsed {len(parsed_logs)} logs\")\n",
    "print(f\"Duplicates removed: {duplicates_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91087f2",
   "metadata": {},
   "source": [
    "##### Classify\n",
    "- Success vs. error (status codes)\n",
    "- Classify logs with 200 status codes as Success 400, 500, etc as error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1d92d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 99, Error: 1, Unknown: 0\n"
     ]
    }
   ],
   "source": [
    " # Classifying the parsed logs as either sucess or error\n",
    "classified_logs = []\n",
    "\n",
    "for log in parsed_logs:\n",
    "    status_code = log['status']\n",
    "\n",
    "    if 200 <= status_code < 400:\n",
    "        log['classification'] = 'Success'\n",
    "    elif 400 <= status_code < 600:\n",
    "        log['classification'] = 'Error'\n",
    "    else:\n",
    "        log['classification'] = 'unknown'\n",
    "    \n",
    "    classified_logs.append(log)\n",
    "\n",
    "\n",
    "# Count success vs errors\n",
    "success_count = sum(1 for log in classified_logs if log['classification'] == 'Success')\n",
    "error_count = sum(1 for log in classified_logs if log['classification'] == 'Error')\n",
    "unknown_count = sum(1 for log in classified_logs if log['classification'] == 'unknown')\n",
    "print(f\"Success: {success_count}, Error: {error_count}, Unknown: {unknown_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e36b4c",
   "metadata": {},
   "source": [
    "##### step 4 - Loading\n",
    "\n",
    "Insert clean records into a local Postgres database (logs.db) using\n",
    "safe transactions (database.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7752504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing database operations...\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# creating a database to store logs\n",
    "def create_database():\n",
    "\n",
    "    # connecting to SQLite database \n",
    "    conn = sqlite3.connect('apache_logs.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Creating logs table to store logs\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS logs (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            ip TEXT,\n",
    "            timestamp TEXT,\n",
    "            method TEXT,\n",
    "            url TEXT,\n",
    "            status INTEGER,\n",
    "            size INTEGER,\n",
    "            classification TEXT,\n",
    "            log_hash TEXT UNIQUE\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Database created\")\n",
    "\n",
    "    \n",
    "    # Create index for faster queries\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_status ON logs(status)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_classification ON logs(classification)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_ip ON logs(ip)')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Database and table created successfully\")\n",
    "\n",
    "def save_classified_logs_to_database(classified_logs):\n",
    "    \n",
    "    if not classified_logs:\n",
    "        print(\"No logs to save\")\n",
    "        return 0\n",
    "    \n",
    "    conn = sqlite3.connect('apache_logs.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    saved_count = 0\n",
    "    duplicate_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for log in classified_logs:\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO logs (ip, timestamp, method, url, protocol, \n",
    "                                status, size, classification, log_hash)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                log['ip'],\n",
    "                log['timestamp'], \n",
    "                log['method'],\n",
    "                log['url'],\n",
    "                log['protocol'],\n",
    "                log['status'],\n",
    "                log['size'],\n",
    "                log['classification'],\n",
    "                log['log_hash']\n",
    "            ))\n",
    "            saved_count += 1\n",
    "            \n",
    "        except sqlite3.IntegrityError:\n",
    "\n",
    "            pass\n",
    "            \n",
    "\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Database save complete:\")\n",
    "    print(f\"  Saved: {saved_count} logs\")\n",
    "    print(f\"  Duplicates skipped: {duplicate_count}\")\n",
    "    print(f\"  Errors: {error_count}\")\n",
    "    \n",
    "    return saved_count\n",
    "\n",
    "\n",
    "\n",
    "# creating a summary report of the database\n",
    "def get_database_summary():\n",
    "    \n",
    "    conn = sqlite3.connect('apache_logs.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Count total logs\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM logs\")\n",
    "    total = cursor.fetchone()[0]\n",
    "    \n",
    "    # Count success logs\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM logs WHERE classification = 'Success'\")\n",
    "    success = cursor.fetchone()[0]\n",
    "    \n",
    "    # Count error logs\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM logs WHERE classification = 'Error'\")\n",
    "    error = cursor.fetchone()[0]\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "\n",
    "    print(f\"Database Summary:\")\n",
    "    print(f\"  Total logs: {total}\")\n",
    "    print(f\"  Success: {success}\")\n",
    "    print(f\"  Error: {error}\")\n",
    "    \n",
    "    return {'total': total, 'success': success, 'error': error}\n",
    "        \n",
    "    \n",
    "\n",
    "# For testing this file directly\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing database operations...\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3605977",
   "metadata": {},
   "source": [
    "##### Step 5 - Summarization\n",
    "- Top IP addresses by request count\n",
    "- Error code summaries (4xx, 5xx)\n",
    "- Hourly traffic pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9019a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_daily_summary():\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('apache_logs.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get overall statistics\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM logs\")\n",
    "    total_logs = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute(\"SELECT COUNT(*) FROM logs WHERE classification = 'Success'\")\n",
    "    success_count = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute(\"SELECT COUNT(*) FROM logs WHERE classification = 'Error'\")\n",
    "    error_count = cursor.fetchone()[0]\n",
    "    \n",
    "    # Get top IP addresses by request count\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT ip, COUNT(*) as count \n",
    "        FROM logs \n",
    "        GROUP BY ip \n",
    "        ORDER BY count DESC \n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    top_ips = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4377dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get error code summaries (4xx, 5xx)\n",
    "def generate_daily_summary():\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('apache_logs.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT status, COUNT(*) as count \n",
    "        FROM logs \n",
    "        WHERE classification = 'Error'\n",
    "        GROUP BY status \n",
    "        ORDER BY count DESC\n",
    "    \"\"\")\n",
    "    error_codes = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45e43e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most requested URLs\n",
    "\n",
    "def generate_daily_summary():\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('apache_logs.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT url, COUNT(*) as count \n",
    "        FROM logs \n",
    "        GROUP BY url \n",
    "        ORDER BY count DESC \n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    top_urls = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cf06e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get traffic by hour \n",
    "def generate_daily_summary():\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('apache_logs.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT substr(timestamp, 13, 2) as hour, COUNT(*) as count\n",
    "        FROM logs \n",
    "        GROUP BY hour \n",
    "        ORDER BY hour\n",
    "    \"\"\")\n",
    "    hourly_traffic = cursor.fetchall()\n",
    "    \n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d73c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "    # Create summary report\n",
    "def generate_daily_summary():\n",
    "\n",
    "    summary = {\n",
    "        'report_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_logs': total_logs,\n",
    "        'success_count': success_count,\n",
    "        'error_count': error_count,\n",
    "        'success_percentage': round((success_count / total_logs * 100), 2) if total_logs > 0 else 0,\n",
    "        'top_ips': top_ips,\n",
    "        'error_codes': error_codes,\n",
    "        'top_urls': top_urls,\n",
    "        'hourly_traffic': hourly_traffic\n",
    "    }\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2672e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_report(summary):\n",
    "    \"\"\"\n",
    "    Save summary as JSON report\n",
    "    \"\"\"\n",
    "    filename = f\"daily_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"JSON report saved: {filename}\")\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a022d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv_report(summary):\n",
    "    \"\"\"\n",
    "    Save summary as CSV report\n",
    "    \"\"\"\n",
    "    filename = f\"daily_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    with open(filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
